{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation includes all six improvements from the MAML++ paper:\n",
    "1. Multi-Step Loss Optimization (MSL)\n",
    "2. Derivative-Order Annealing (DA)\n",
    "3. Per-Step Batch Normalization Running Statistics (BNRS)\n",
    "4. Per-Step Batch Normalization Weights and Biases (BNWB)\n",
    "5. Learning Per-Layer Per-Step Learning Rates (LSLR)\n",
    "6. Cosine Annealing of Meta-Optimizer (CA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cpu\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import glob, random\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import math\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"DEVICE = {device}\")\n",
    "\n",
    "# Fix random seeds\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for MAML++\n",
    "n_way = 3\n",
    "k_shot = 5\n",
    "q_query = 5\n",
    "input_dim = 1280\n",
    "train_inner_train_step = 5\n",
    "val_inner_train_step = 5\n",
    "inner_lr = 0.01  # Will be learned per-layer per-step in MAML++\n",
    "meta_lr = 0.001\n",
    "meta_batch_size = 16\n",
    "max_epoch = 30\n",
    "eval_batches = 20\n",
    "\n",
    "# MAML++ specific hyperparameters\n",
    "use_first_order_epochs = max_epoch // 2  # First 15 epochs use first-order gradients (DA)\n",
    "step_weights_initial = [1.0] * (train_inner_train_step + 1)  # For Multi-Step Loss Optimization (MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML++ IMPROVEMENT 1: Enhanced Batch Normalization \n",
    "class PerStepBatchNorm1d(nn.Module):\n",
    "    \"\"\"MAML++ Improvement: Per-Step Batch Normalization (BNRS + BNWB)\n",
    "    - BNRS: Per-step running statistics\n",
    "    - BNWB: Per-step weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_steps, momentum=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_steps = num_steps\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        \n",
    "        # BNWB: Per-step weights and biases (including step 0)\n",
    "        self.weight = nn.Parameter(torch.ones(num_steps + 1, num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_steps + 1, num_features))\n",
    "        \n",
    "        # BNRS: Per-step running statistics (including step 0)\n",
    "        self.register_buffer('running_mean', torch.zeros(num_steps + 1, num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_steps + 1, num_features))\n",
    "        \n",
    "    def forward(self, x, step=0):\n",
    "        step = min(step, self.num_steps)\n",
    "            \n",
    "        if self.training:\n",
    "            # Use batch statistics during training\n",
    "            batch_mean = x.mean(dim=0, keepdim=False)\n",
    "            batch_var = x.var(dim=0, unbiased=False, keepdim=False)\n",
    "            \n",
    "            # Update running statistics for this step\n",
    "            with torch.no_grad():\n",
    "                self.running_mean[step] = (1 - self.momentum) * self.running_mean[step] + self.momentum * batch_mean\n",
    "                self.running_var[step] = (1 - self.momentum) * self.running_var[step] + self.momentum * batch_var\n",
    "            \n",
    "            mean = batch_mean\n",
    "            var = batch_var\n",
    "        else:\n",
    "            # Use running statistics during evaluation\n",
    "            mean = self.running_mean[step]\n",
    "            var = self.running_var[step]\n",
    "        \n",
    "        # Normalize and apply per-step weights and biases\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight[step] * x_norm + self.bias[step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML++ IMPROVEMENT 2: Enhanced Classifier with Per-Layer Learning Rates\n",
    "class MalwarePlusPlusClassifier(nn.Module):\n",
    "    \"\"\"MAML++ Enhanced Classifier with multiple improvements\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=3, num_inner_steps=5):\n",
    "        super(MalwarePlusPlusClassifier, self).__init__()\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        \n",
    "        # Original network layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.fc4 = nn.Linear(hidden_dim//2, output_dim)\n",
    "        \n",
    "        # MAML++ IMPROVEMENT: Per-step batch normalization (BNRS + BNWB)\n",
    "        self.bn1 = PerStepBatchNorm1d(hidden_dim, num_inner_steps)\n",
    "        self.bn2 = PerStepBatchNorm1d(hidden_dim, num_inner_steps)\n",
    "        self.bn3 = PerStepBatchNorm1d(hidden_dim//2, num_inner_steps)\n",
    "        \n",
    "        # MAML++ IMPROVEMENT: Learnable per-layer per-step learning rates (LSLR)\n",
    "        # Each layer has its own learning rate for each step\n",
    "        self.layer_lrs = nn.ParameterDict({\n",
    "            # FC layers use relatively larger learning rates\n",
    "            'fc1': nn.Parameter(torch.tensor([0.01, 0.008, 0.006, 0.004, 0.002][:num_inner_steps])),\n",
    "            'fc2': nn.Parameter(torch.tensor([0.005, 0.004, 0.003, 0.002, 0.001][:num_inner_steps])),\n",
    "            'fc3': nn.Parameter(torch.tensor([0.003, 0.002, 0.002, 0.001, 0.001][:num_inner_steps])),\n",
    "            'fc4': nn.Parameter(torch.tensor([0.008, 0.006, 0.004, 0.003, 0.002][:num_inner_steps])),  \n",
    "            # BN layers use smaller learning rates\n",
    "            'bn1': nn.Parameter(torch.tensor([0.001, 0.0008, 0.0006, 0.0004, 0.0002][:num_inner_steps])),\n",
    "            'bn2': nn.Parameter(torch.tensor([0.001, 0.0008, 0.0006, 0.0004, 0.0002][:num_inner_steps])),\n",
    "            'bn3': nn.Parameter(torch.tensor([0.001, 0.0008, 0.0006, 0.0004, 0.0002][:num_inner_steps]))\n",
    "        })\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x, step=0):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x, step)  # Use step-specific BN\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x, step)  # Use step-specific BN\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x, step)  # Use step-specific BN\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def functional_forward(self, x, params, step=0):\n",
    "        \"\"\"Forward pass using custom parameters with step-aware BN\"\"\"\n",
    "        # FC1\n",
    "        x = F.linear(x, params.get('fc1.weight', self.fc1.weight), \n",
    "                    params.get('fc1.bias', self.fc1.bias))\n",
    "        \n",
    "        # BN1 - Use step-specific parameters\n",
    "        step_idx = min(step, self.num_inner_steps)\n",
    "        if f'bn1.weight' in params:\n",
    "            bn1_weight = params[f'bn1.weight'][step_idx]\n",
    "            bn1_bias = params[f'bn1.bias'][step_idx]\n",
    "        else:\n",
    "            bn1_weight = self.bn1.weight[step_idx]\n",
    "            bn1_bias = self.bn1.bias[step_idx]\n",
    "            \n",
    "        bn1_running_mean = self.bn1.running_mean[step_idx]\n",
    "        bn1_running_var = self.bn1.running_var[step_idx]\n",
    "        \n",
    "        x = F.batch_norm(x, bn1_running_mean, bn1_running_var, \n",
    "                        bn1_weight, bn1_bias, training=self.training, eps=1e-5)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training, p=0.3)\n",
    "        \n",
    "        # FC2\n",
    "        x = F.linear(x, params.get('fc2.weight', self.fc2.weight), \n",
    "                    params.get('fc2.bias', self.fc2.bias))\n",
    "        \n",
    "        # BN2\n",
    "        if f'bn2.weight' in params:\n",
    "            bn2_weight = params[f'bn2.weight'][step_idx]\n",
    "            bn2_bias = params[f'bn2.bias'][step_idx]\n",
    "        else:\n",
    "            bn2_weight = self.bn2.weight[step_idx]\n",
    "            bn2_bias = self.bn2.bias[step_idx]\n",
    "            \n",
    "        bn2_running_mean = self.bn2.running_mean[step_idx]\n",
    "        bn2_running_var = self.bn2.running_var[step_idx]\n",
    "        \n",
    "        x = F.batch_norm(x, bn2_running_mean, bn2_running_var,\n",
    "                        bn2_weight, bn2_bias, training=self.training, eps=1e-5)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training, p=0.3)\n",
    "        \n",
    "        # FC3\n",
    "        x = F.linear(x, params.get('fc3.weight', self.fc3.weight), \n",
    "                    params.get('fc3.bias', self.fc3.bias))\n",
    "        \n",
    "        # BN3\n",
    "        if f'bn3.weight' in params:\n",
    "            bn3_weight = params[f'bn3.weight'][step_idx]\n",
    "            bn3_bias = params[f'bn3.bias'][step_idx]\n",
    "        else:\n",
    "            bn3_weight = self.bn3.weight[step_idx]\n",
    "            bn3_bias = self.bn3.bias[step_idx]\n",
    "            \n",
    "        bn3_running_mean = self.bn3.running_mean[step_idx]\n",
    "        bn3_running_var = self.bn3.running_var[step_idx]\n",
    "        \n",
    "        x = F.batch_norm(x, bn3_running_mean, bn3_running_var,\n",
    "                        bn3_weight, bn3_bias, training=self.training, eps=1e-5)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # FC4\n",
    "        x = F.linear(x, params.get('fc4.weight', self.fc4.weight), \n",
    "                    params.get('fc4.bias', self.fc4.bias))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def create_malware_label(k_shot, q_query):\n",
    "    \"\"\"Create labels for calculating accuracy in test phase.\"\"\"\n",
    "    n_way = 3  # 2 abnormal + 1 normal\n",
    "    labels = []\n",
    "    for class_idx in range(n_way):\n",
    "        class_labels = [class_idx] * (k_shot + q_query)\n",
    "        labels.extend(class_labels)\n",
    "    return torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def create_label(n_way, k_shot):\n",
    "    \"\"\"Create labels for support set and query set.\"\"\"\n",
    "    return torch.arange(n_way).repeat_interleave(k_shot).long()\n",
    "\n",
    "def calculate_accuracy(logits, labels):\n",
    "    \"\"\"utility function for accuracy calculation\"\"\"\n",
    "    acc = np.asarray(\n",
    "        [(torch.argmax(logits, -1).cpu().numpy() == labels.cpu().numpy())]\n",
    "    ).mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML++ IMPROVEMENT 3: Cosine Annealing Scheduler\n",
    "class CosineAnnealingLR:\n",
    "    \"\"\"MAML++ Improvement: Cosine Annealing for Meta-Optimizer (CA)\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.last_epoch = last_epoch\n",
    "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        \n",
    "        for param_group, base_lr in zip(self.optimizer.param_groups, self.base_lrs):\n",
    "            param_group['lr'] = self.eta_min + (base_lr - self.eta_min) * \\\n",
    "                               (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "                               \n",
    "    def get_lr(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (using simplified version for demonstration)\n",
    "class MalwareDetection(Dataset):\n",
    "    def __init__(self, data_structure_file, split='train', k_shot=1, q_query=5):\n",
    "        with open(data_structure_file, 'r') as f:\n",
    "            self.data_structure = json.load(f)\n",
    "        \n",
    "        self.split = split\n",
    "        self.classes = list(self.data_structure[split].keys())\n",
    "        self.k_shot = k_shot\n",
    "        self.q_query = q_query\n",
    "        self.normal_class = 'benign'\n",
    "        \n",
    "        self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        min_samples = self.k_shot + self.q_query\n",
    "        for cls, files in self.data_structure[self.split].items():\n",
    "            if len(files) < min_samples:\n",
    "                print(f\"Warning: only {len(files)} samples in class '{cls}' for split '{self.split}'. Required: {min_samples}. Will sample with replacement.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        np.random.seed(42 + idx)\n",
    "\n",
    "        fraud_classes = [cls for cls in self.classes if cls != self.normal_class]\n",
    "        \n",
    "        if len(fraud_classes) >= 2:\n",
    "            selected_frauds = np.random.choice(fraud_classes, 2, replace=False)\n",
    "            task_classes = list(selected_frauds) + [self.normal_class]\n",
    "        elif len(fraud_classes) == 1:\n",
    "            if self.split == 'test':\n",
    "                task_classes = fraud_classes + [self.normal_class]\n",
    "            else:\n",
    "                task_classes = fraud_classes + fraud_classes + [self.normal_class]\n",
    "        else:\n",
    "            raise ValueError(f\"No fraud classes available in {self.split} split\")\n",
    "        \n",
    "        task_data = []\n",
    "        for cls in task_classes:\n",
    "            class_files = self.data_structure[self.split][cls]\n",
    "            \n",
    "            if len(class_files) >= self.k_shot + self.q_query:\n",
    "                selected_files = np.random.choice(class_files, \n",
    "                                                self.k_shot + self.q_query, \n",
    "                                                replace=False)\n",
    "            else:\n",
    "                selected_files = np.random.choice(class_files, \n",
    "                                                self.k_shot + self.q_query, \n",
    "                                                replace=True)\n",
    "            \n",
    "            class_features = []\n",
    "            for file_path in selected_files:\n",
    "                corrected_path = self._fix_file_path(file_path)\n",
    "                \n",
    "                try:\n",
    "                    features = np.load(corrected_path)\n",
    "                    if features.ndim > 1:\n",
    "                        features = features.flatten()\n",
    "                    class_features.append(features)\n",
    "                except Exception as e:\n",
    "                    if idx == 0:\n",
    "                        print(f\"Error loading {corrected_path}: {e}\")\n",
    "                    class_features.append(np.zeros(1280))\n",
    "            \n",
    "            task_data.append(torch.tensor(np.array(class_features), dtype=torch.float32))\n",
    "        \n",
    "        return torch.stack(task_data)\n",
    "    \n",
    "    def _fix_file_path(self, original_path):\n",
    "        if os.path.exists(original_path):\n",
    "            return original_path\n",
    "        \n",
    "        possible_prefixes = ['../', '../../', './']\n",
    "        for prefix in possible_prefixes:\n",
    "            new_path = os.path.join(prefix, original_path)\n",
    "            if os.path.exists(new_path):\n",
    "                return os.path.abspath(new_path)\n",
    "        \n",
    "        return original_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        fraud_classes = [cls for cls in self.classes if cls != self.normal_class]\n",
    "        if len(fraud_classes) >= 2:\n",
    "            from math import comb\n",
    "            return comb(len(fraud_classes), 2) * 100\n",
    "        else:\n",
    "            return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_batch(meta_batch_size, k_shot, q_query, data_loader, iterator):\n",
    "    \"\"\"Get meta batch function\"\"\"\n",
    "    data = []\n",
    "    for _ in range(meta_batch_size):\n",
    "        try:\n",
    "            task_data = next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(data_loader)\n",
    "            task_data = next(iterator)\n",
    "        \n",
    "        task_data = task_data.squeeze(0)\n",
    "        task_data = task_data.view(-1, task_data.size(-1))\n",
    "        data.append(task_data)\n",
    "    \n",
    "    return torch.stack(data).to(device), iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML++ MAIN ALGORITHM WITH ALL IMPROVEMENTS\n",
    "def MAMLPlusPlusSolver(\n",
    "    model,\n",
    "    optimizer,\n",
    "    x,\n",
    "    n_way,\n",
    "    k_shot,\n",
    "    q_query,\n",
    "    loss_fn,\n",
    "    inner_train_step,\n",
    "    train,\n",
    "    epoch=0,\n",
    "    step_weights=None,\n",
    "    use_first_order_epochs=15,\n",
    "    return_labels=False,\n",
    "):\n",
    "    \"\"\"MAML++ Algorithm with all six improvements\"\"\"\n",
    "    criterion = loss_fn\n",
    "    task_loss = []\n",
    "    task_acc = []\n",
    "    labels = []\n",
    "    \n",
    "    # MAML++ IMPROVEMENT 4: Derivative-Order Annealing (DA)\n",
    "    use_second_order = epoch >= use_first_order_epochs\n",
    "    \n",
    "    if step_weights is None:\n",
    "        step_weights = [1.0] * (inner_train_step + 1)\n",
    "    \n",
    "    for meta_batch in x:\n",
    "        # Split support and query sets\n",
    "        support_set = meta_batch[: n_way * k_shot]\n",
    "        query_set = meta_batch[n_way * k_shot :]\n",
    "\n",
    "        # Copy the params for inner loop\n",
    "        fast_weights = OrderedDict(model.named_parameters())\n",
    "        \n",
    "        # MAML++ IMPROVEMENT 5: Multi-Step Loss Optimization (MSL)\n",
    "        # Store losses from all steps for multi-step optimization\n",
    "        step_losses = []\n",
    "        \n",
    "        ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "        for inner_step in range(inner_train_step):\n",
    "            train_label = create_label(n_way, k_shot).to(device)\n",
    "            \n",
    "            # Forward pass with step-aware batch normalization\n",
    "            logits = model.functional_forward(support_set, fast_weights, step=inner_step)\n",
    "            loss = criterion(logits, train_label)\n",
    "            \n",
    "            # Calculate gradients with appropriate order\n",
    "            if use_second_order and train:\n",
    "                grads = torch.autograd.grad(loss, fast_weights.values(), \n",
    "                                          create_graph=True, allow_unused=True, retain_graph=True)\n",
    "            else:\n",
    "                grads = torch.autograd.grad(loss, fast_weights.values(), \n",
    "                                          create_graph=False, allow_unused=True, retain_graph=True)\n",
    "\n",
    "            # MAML++ IMPROVEMENT 6: Per-Layer Per-Step Learning Rates (LSLR)\n",
    "            # Update fast_weights using learned learning rates\n",
    "            updated_params = OrderedDict()\n",
    "            \n",
    "            for (name, param), grad in zip(fast_weights.items(), grads):\n",
    "                if grad is None:\n",
    "                    # Handle unused parameters\n",
    "                    updated_params[name] = param\n",
    "                    continue\n",
    "                    \n",
    "                # Determine which learning rate to use\n",
    "                layer_name = name.split('.')[0]  # e.g., 'fc1', 'bn1'\n",
    "                \n",
    "                if layer_name in model.layer_lrs and inner_step < len(model.layer_lrs[layer_name]):\n",
    "                    lr = torch.abs(model.layer_lrs[layer_name][inner_step])  # Ensure positive LR\n",
    "                    # Added clamping to keep learning rates in a reasonable range\n",
    "                    lr = torch.clamp(lr, min=1e-6, max=0.1)\n",
    "                else:\n",
    "                    lr = 0.001  # Fallback learning rate\n",
    "\n",
    "                # Debug info: print learning rate every 50 steps\n",
    "                if inner_step == 0 and hasattr(MAMLPlusPlusSolver, '_debug_counter'):\n",
    "                    MAMLPlusPlusSolver._debug_counter = getattr(MAMLPlusPlusSolver, '_debug_counter', 0) + 1\n",
    "                    if MAMLPlusPlusSolver._debug_counter % 50 == 0:\n",
    "                        print(f\"Layer {layer_name} step {inner_step} LR: {lr:.6f}\")\n",
    "                \n",
    "                updated_params[name] = param - lr * grad\n",
    "            \n",
    "            fast_weights = updated_params\n",
    "            \n",
    "            # MSL: Compute query loss for this step\n",
    "            if not return_labels:\n",
    "                val_label = create_label(n_way, q_query).to(device)\n",
    "                query_logits = model.functional_forward(query_set, fast_weights, step=inner_step)\n",
    "                query_loss = criterion(query_logits, val_label)\n",
    "                step_losses.append(query_loss * step_weights[inner_step])\n",
    "\n",
    "        ### ---------- FINAL STEP EVALUATION ---------- ###\n",
    "        # MSL: Compute query loss for this step\n",
    "        if not return_labels:\n",
    "            # Evaluate final step\n",
    "            val_label = create_label(n_way, q_query).to(device)\n",
    "            final_logits = model.functional_forward(query_set, fast_weights, step=inner_train_step)\n",
    "            final_loss = criterion(final_logits, val_label)\n",
    "            step_losses.append(final_loss * step_weights[inner_train_step])\n",
    "            \n",
    "            # MSL: Combine all step losses\n",
    "            total_loss = sum(step_losses) / sum(step_weights)\n",
    "            task_loss.append(total_loss)\n",
    "            task_acc.append(calculate_accuracy(final_logits, val_label))  # 現在用 final_logits\n",
    "        else:\n",
    "            # Testing mode\n",
    "            logits = model.functional_forward(query_set, fast_weights, step=inner_train_step)\n",
    "            labels.extend(torch.argmax(logits, -1).cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return labels\n",
    "\n",
    "    # Update outer loop\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    meta_batch_loss = torch.stack(task_loss).mean()\n",
    "    if train:\n",
    "        meta_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    task_acc = np.mean(task_acc)\n",
    "    return meta_batch_loss, task_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTIVE STEP WEIGHTS FOR MSL\n",
    "def get_step_weights(epoch, max_epochs, num_steps):\n",
    "    \"\"\"MAML++ MSL: Adaptive step weights with annealing\n",
    "    Early epochs: equal weights for all steps\n",
    "    Later epochs: higher weight for final steps\n",
    "    \"\"\"\n",
    "    progress = epoch / max_epochs\n",
    "    weights = []\n",
    "    \n",
    "    for i in range(num_steps + 1):  # +1 for final evaluation step\n",
    "        if i == num_steps:  # Final step - stronger weight increase\n",
    "            # From 1.0 to 5.0\n",
    "            weight = 1.0 + progress * 4.0  \n",
    "        else:\n",
    "            # More aggressive early step weight decay\n",
    "            decay_factor = (num_steps - i) / num_steps\n",
    "            weight = 1.0 - progress * 0.8 * decay_factor  # Originally 0.5, changed to 0.8\n",
    "\n",
    "        # Ensure weights don't get too small\n",
    "        weights.append(max(weight, 0.1))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAML++ Model parameters: 434726\n",
      "Using first-order gradients for first 15 epochs\n",
      "Meta-learning rate will be annealed from 0.001 to 1e-05\n"
     ]
    }
   ],
   "source": [
    "# SETUP TRAINING WITH MAML++ IMPROVEMENTS\n",
    "\n",
    "# Check if data file exists\n",
    "data_file = '../malware_data_structure.json'\n",
    "if os.path.exists(data_file):\n",
    "    # Prepare datasets and dataloaders\n",
    "    train_dataset = MalwareDetection(data_file, 'train', k_shot, q_query)\n",
    "    val_dataset = MalwareDetection(data_file, 'val', k_shot, q_query)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Create MAML++ model with enhanced features\n",
    "    meta_model = MalwarePlusPlusClassifier(\n",
    "        input_dim=input_dim, \n",
    "        num_inner_steps=train_inner_train_step\n",
    "    ).to(device)\n",
    "\n",
    "    # MAML++ IMPROVEMENT: Meta-optimizer with cosine annealing (CA)\n",
    "    optimizer = torch.optim.Adam(meta_model.parameters(), lr=meta_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=meta_lr * 0.01)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"MAML++ Model parameters: {sum(p.numel() for p in meta_model.parameters())}\")\n",
    "    print(f\"Using first-order gradients for first {use_first_order_epochs} epochs\")\n",
    "    print(f\"Meta-learning rate will be annealed from {meta_lr} to {meta_lr * 0.01}\")\n",
    "else:\n",
    "    print(f\"Data file {data_file} not found. Please make sure the data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MAML++ training\n",
      "Improvements included:\n",
      "1. Multi-Step Loss Optimization (MSL) - ✓\n",
      "2. Derivative-Order Annealing (DA) - ✓\n",
      "3. Per-Step Batch Normalization (BNRS + BNWB) - ✓\n",
      "4. Per-Layer Per-Step Learning Rates (LSLR) - ✓\n",
      "5. Cosine Annealing Meta-Optimizer (CA) - ✓\n",
      "------------------------------------------------------------\n",
      "Epoch 1/30\n",
      "Meta-LR: 0.001000, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.842\tAccuracy: 54.382%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.111%\n",
      "--------------------------------------------------\n",
      "Epoch 2/30\n",
      "Meta-LR: 0.001000, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.747\tAccuracy: 58.589%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 53.472%\n",
      "--------------------------------------------------\n",
      "Epoch 3/30\n",
      "Meta-LR: 0.000997, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.716\tAccuracy: 59.899%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 49.861%\n",
      "--------------------------------------------------\n",
      "Epoch 4/30\n",
      "Meta-LR: 0.000989, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.684\tAccuracy: 61.546%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 50.486%\n",
      "--------------------------------------------------\n",
      "Epoch 5/30\n",
      "Meta-LR: 0.000976, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.666\tAccuracy: 62.796%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:00<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.917%\n",
      "--------------------------------------------------\n",
      "Epoch 6/30\n",
      "Meta-LR: 0.000957, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.647\tAccuracy: 63.723%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 53.958%\n",
      "--------------------------------------------------\n",
      "Epoch 7/30\n",
      "Meta-LR: 0.000934, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.632\tAccuracy: 65.255%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.944%\n",
      "--------------------------------------------------\n",
      "Epoch 8/30\n",
      "Meta-LR: 0.000905, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.607\tAccuracy: 66.505%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 50.139%\n",
      "--------------------------------------------------\n",
      "Epoch 9/30\n",
      "Meta-LR: 0.000873, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:14<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.586\tAccuracy: 67.440%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:00<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 53.750%\n",
      "--------------------------------------------------\n",
      "Epoch 10/30\n",
      "Meta-LR: 0.000836, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.571\tAccuracy: 68.441%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.111%\n",
      "--------------------------------------------------\n",
      "Epoch 11/30\n",
      "Meta-LR: 0.000796, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:14<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.550\tAccuracy: 69.483%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.250%\n",
      "--------------------------------------------------\n",
      "Epoch 12/30\n",
      "Meta-LR: 0.000753, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.542\tAccuracy: 70.249%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.569%\n",
      "--------------------------------------------------\n",
      "Epoch 13/30\n",
      "Meta-LR: 0.000706, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.517\tAccuracy: 71.801%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 50.486%\n",
      "--------------------------------------------------\n",
      "Epoch 14/30\n",
      "Meta-LR: 0.000658, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:15<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.496\tAccuracy: 73.353%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:00<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.528%\n",
      "--------------------------------------------------\n",
      "Epoch 15/30\n",
      "Meta-LR: 0.000608, Using 1st-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:14<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.487\tAccuracy: 74.160%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 50.000%\n",
      "--------------------------------------------------\n",
      "Epoch 16/30\n",
      "Meta-LR: 0.000557, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.462\tAccuracy: 76.257%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:00<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 53.472%\n",
      "--------------------------------------------------\n",
      "Epoch 17/30\n",
      "Meta-LR: 0.000505, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.441\tAccuracy: 77.466%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.708%\n",
      "--------------------------------------------------\n",
      "Epoch 18/30\n",
      "Meta-LR: 0.000453, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.426\tAccuracy: 78.300%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.222%\n",
      "--------------------------------------------------\n",
      "Epoch 19/30\n",
      "Meta-LR: 0.000402, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.404\tAccuracy: 79.966%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:00<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 53.472%\n",
      "--------------------------------------------------\n",
      "Epoch 20/30\n",
      "Meta-LR: 0.000352, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.388\tAccuracy: 80.780%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 53.264%\n",
      "--------------------------------------------------\n",
      "Epoch 21/30\n",
      "Meta-LR: 0.000304, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.366\tAccuracy: 82.392%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.292%\n",
      "--------------------------------------------------\n",
      "Epoch 22/30\n",
      "Meta-LR: 0.000258, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.350\tAccuracy: 83.401%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.736%\n",
      "--------------------------------------------------\n",
      "Epoch 23/30\n",
      "Meta-LR: 0.000214, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.336\tAccuracy: 84.066%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.083%\n",
      "--------------------------------------------------\n",
      "Epoch 24/30\n",
      "Meta-LR: 0.000174, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.318\tAccuracy: 85.363%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.528%\n",
      "--------------------------------------------------\n",
      "Epoch 25/30\n",
      "Meta-LR: 0.000137, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.305\tAccuracy: 86.183%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.111%\n",
      "--------------------------------------------------\n",
      "Epoch 26/30\n",
      "Meta-LR: 0.000105, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.295\tAccuracy: 86.835%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.222%\n",
      "--------------------------------------------------\n",
      "Epoch 27/30\n",
      "Meta-LR: 0.000076, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.278\tAccuracy: 87.641%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.708%\n",
      "--------------------------------------------------\n",
      "Epoch 28/30\n",
      "Meta-LR: 0.000053, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:20<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.274\tAccuracy: 87.675%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.986%\n",
      "--------------------------------------------------\n",
      "Epoch 29/30\n",
      "Meta-LR: 0.000034, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.266\tAccuracy: 88.172%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:01<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.639%\n",
      "--------------------------------------------------\n",
      "Epoch 30/30\n",
      "Meta-LR: 0.000021, Using 2nd-order gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:19<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.259\tAccuracy: 88.730%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 6/6 [00:00<00:00,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 52.569%\n",
      "--------------------------------------------------\n",
      "MAML++ Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MAML++ TRAINING LOOP (only if data is available)\n",
    "if 'meta_model' in locals():\n",
    "    train_iter = iter(train_loader)\n",
    "    val_iter = iter(val_loader)\n",
    "\n",
    "    print(\"Starting MAML++ training\")\n",
    "    print(\"Improvements included:\")\n",
    "    print(\"1. Multi-Step Loss Optimization (MSL) - ✓\")\n",
    "    print(\"2. Derivative-Order Annealing (DA) - ✓\")\n",
    "    print(\"3. Per-Step Batch Normalization (BNRS + BNWB) - ✓\")\n",
    "    print(\"4. Per-Layer Per-Step Learning Rates (LSLR) - ✓\")\n",
    "    print(\"5. Cosine Annealing Meta-Optimizer (CA) - ✓\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        print(f\"Epoch {epoch+1}/{max_epoch}\")\n",
    "        \n",
    "        # Get adaptive step weights for MSL\n",
    "        step_weights = get_step_weights(epoch, max_epoch, train_inner_train_step)\n",
    "        \n",
    "        # Show current learning rate (CA)\n",
    "        current_lr = scheduler.get_lr()[0]\n",
    "        print(f\"Meta-LR: {current_lr:.6f}, Using {'2nd' if epoch >= use_first_order_epochs else '1st'}-order gradients\")\n",
    "        \n",
    "        # Training\n",
    "        train_meta_loss = []\n",
    "        train_acc = []\n",
    "        \n",
    "        for train_step in tqdm(range(len(train_loader) // meta_batch_size), desc=\"Training\"):\n",
    "            x, train_iter = get_meta_batch(\n",
    "                meta_batch_size, k_shot, q_query, train_loader, train_iter\n",
    "            )\n",
    "            \n",
    "            meta_loss, acc = MAMLPlusPlusSolver(\n",
    "                meta_model,\n",
    "                optimizer,\n",
    "                x,\n",
    "                n_way,\n",
    "                k_shot,\n",
    "                q_query,\n",
    "                loss_fn,\n",
    "                inner_train_step=train_inner_train_step,\n",
    "                train=True,\n",
    "                epoch=epoch,\n",
    "                step_weights=step_weights,\n",
    "                use_first_order_epochs=use_first_order_epochs\n",
    "            )\n",
    "            \n",
    "            train_meta_loss.append(meta_loss.item())\n",
    "            train_acc.append(acc)\n",
    "        \n",
    "        print(f\"Loss: {np.mean(train_meta_loss):.3f}\\tAccuracy: {np.mean(train_acc)*100:.3f}%\")\n",
    "        \n",
    "        # Validation\n",
    "        val_acc = []\n",
    "        for eval_step in tqdm(range(min(eval_batches, len(val_loader) // meta_batch_size)), desc=\"Validation\"):\n",
    "            x, val_iter = get_meta_batch(\n",
    "                meta_batch_size, k_shot, q_query, val_loader, val_iter\n",
    "            )\n",
    "            \n",
    "            _, acc = MAMLPlusPlusSolver(\n",
    "                meta_model,\n",
    "                optimizer,\n",
    "                x,\n",
    "                n_way,\n",
    "                k_shot,\n",
    "                q_query,\n",
    "                loss_fn,\n",
    "                inner_train_step=val_inner_train_step,\n",
    "                train=False,\n",
    "                epoch=epoch,\n",
    "                step_weights=step_weights,\n",
    "                use_first_order_epochs=use_first_order_epochs\n",
    "            )\n",
    "            val_acc.append(acc)\n",
    "        \n",
    "        print(f\"Validation accuracy: {np.mean(val_acc)*100:.3f}%\")\n",
    "        \n",
    "        # Update learning rate scheduler (CA)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"MAML++ Training Complete!\")\n",
    "else:\n",
    "    print(\"Training skipped due to missing data file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAML++ model saved as malware_maml_plus_plus_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained MAML++ model\n",
    "if 'meta_model' in locals():\n",
    "    torch.save({\n",
    "        'model_state_dict': meta_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.__dict__,\n",
    "        'hyperparameters': {\n",
    "            'n_way': n_way,\n",
    "            'k_shot': k_shot,\n",
    "            'q_query': q_query,\n",
    "            'input_dim': input_dim,\n",
    "            'meta_lr': meta_lr,\n",
    "            'train_inner_train_step': train_inner_train_step,\n",
    "            'use_first_order_epochs': use_first_order_epochs\n",
    "        },\n",
    "        'improvements': {\n",
    "            'MSL': 'Multi-Step Loss Optimization',\n",
    "            'DA': 'Derivative-Order Annealing', \n",
    "            'BNRS': 'Per-Step Batch Normalization Running Statistics',\n",
    "            'BNWB': 'Per-Step Batch Normalization Weights and Biases',\n",
    "            'LSLR': 'Learning Per-Layer Per-Step Learning Rates',\n",
    "            'CA': 'Cosine Annealing of Meta-Optimizer'\n",
    "        }\n",
    "    }, 'malware_maml++_model.pth')\n",
    "\n",
    "    print(\"MAML++ model saved as malware_maml_plus_plus_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed81572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL MAML++ TESTING\n",
      "============================================================\n",
      "Starting MAML++ testing and accuracy calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing MAML++ with Accuracy:  45%|████▌     | 9/20 [00:00<00:00, 83.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/20 - Task Accuracy: 0.8667\n",
      "Batch 6/20 - Task Accuracy: 0.5333\n",
      "Batch 11/20 - Task Accuracy: 0.5333\n",
      "Batch 16/20 - Task Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing MAML++ with Accuracy: 100%|██████████| 20/20 [00:00<00:00, 89.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAML++ Final Test Results:\n",
      "Average Test Task Accuracy: 61.000% ± 11.010%\n",
      "Best Task Accuracy: 86.667%\n",
      "Worst Task Accuracy: 40.000%\n",
      "MAML++ test results saved as malware_maml_plus_plus_predictions.csv\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.89      0.89      0.89       100\n",
      "     Class 1       0.46      0.48      0.47       100\n",
      "     Class 2       0.48      0.46      0.47       100\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.61      0.61      0.61       300\n",
      "weighted avg       0.61      0.61      0.61       300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[89  6  5]\n",
      " [ 7 48 45]\n",
      " [ 4 50 46]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MAML++ Testing with accuracy calculation\n",
    "def test_maml_plus_plus_model(model, test_data_path_or_dataset, inner_train_step=500, epoch=30):\n",
    "    \"\"\"\n",
    "    MAML++ Test function that returns predicted and true labels for accuracy calculation\n",
    "    3-way tasks: 2 abnormal + 1 normal\n",
    "    \"\"\"\n",
    "    test_dataset = MalwareDetection(test_data_path_or_dataset, 'test', k_shot, q_query)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    test_iter = iter(test_loader)\n",
    "    \n",
    "    test_batches = min(20, len(test_loader))\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "    task_accuracies = []\n",
    "\n",
    "    print(\"Starting MAML++ testing and accuracy calculation...\")\n",
    "\n",
    "    # Fix random seed for consistent label generation\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Get adaptive step weights for testing (same as final training epoch)\n",
    "    step_weights = get_step_weights(epoch-1, max_epoch, inner_train_step)\n",
    "    \n",
    "    for batch_idx in tqdm(range(test_batches), desc=\"Testing MAML++ with Accuracy\"):\n",
    "        x, test_iter = get_meta_batch(1, k_shot, q_query, test_loader, test_iter)\n",
    "\n",
    "        # Check the actual task dimensions\n",
    "        batch_size, total_samples, feature_dim = x.shape\n",
    "        actual_n_way = total_samples // (k_shot + q_query)\n",
    "\n",
    "        # 3-way task query set labels\n",
    "        task_true_labels = []\n",
    "        for class_idx in range(3):\n",
    "            task_true_labels.extend([class_idx] * q_query)\n",
    "\n",
    "        # Get model predictions using MAML++\n",
    "        predicted_labels = MAMLPlusPlusSolver(\n",
    "            model,\n",
    "            optimizer, \n",
    "            x,\n",
    "            3,  \n",
    "            k_shot,\n",
    "            q_query,\n",
    "            loss_fn,\n",
    "            inner_train_step=inner_train_step,\n",
    "            train=False,\n",
    "            epoch=epoch,  # Use final epoch settings\n",
    "            step_weights=step_weights,\n",
    "            use_first_order_epochs=use_first_order_epochs,\n",
    "            return_labels=True,\n",
    "        )\n",
    "\n",
    "        # Calculate current task accuracy\n",
    "        task_true = np.array(task_true_labels)\n",
    "        task_pred = np.array(predicted_labels)\n",
    "        task_acc = (task_true == task_pred).mean()\n",
    "        task_accuracies.append(task_acc)\n",
    "\n",
    "        # Collect all labels\n",
    "        all_predicted_labels.extend(predicted_labels)\n",
    "        all_true_labels.extend(task_true_labels)\n",
    "\n",
    "        if batch_idx % 5 == 0:  # Print every 5 batches\n",
    "            print(f\"Batch {batch_idx+1}/{test_batches} - Task Accuracy: {task_acc:.4f}\")\n",
    "    \n",
    "    return all_predicted_labels, all_true_labels, task_accuracies\n",
    "\n",
    "# Execute MAML++ testing with accuracy calculation\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MAML++ TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_predicted_labels, test_true_labels, test_task_accuracies = test_maml_plus_plus_model(\n",
    "    meta_model, \n",
    "    '../malware_data_structure.json',\n",
    "    inner_train_step=val_inner_train_step,  # Use validation inner steps for testing\n",
    "    epoch=max_epoch\n",
    ")\n",
    "\n",
    "average_test_accuracy = np.mean(test_task_accuracies)\n",
    "std_test_accuracy = np.std(test_task_accuracies)\n",
    "\n",
    "print(f\"MAML++ Final Test Results:\")\n",
    "print(f\"Average Test Task Accuracy: {average_test_accuracy*100:.3f}% ± {std_test_accuracy*100:.3f}%\")\n",
    "print(f\"Best Task Accuracy: {np.max(test_task_accuracies)*100:.3f}%\")\n",
    "print(f\"Worst Task Accuracy: {np.min(test_task_accuracies)*100:.3f}%\")\n",
    "\n",
    "# Save MAML++ test results\n",
    "results_df = pd.DataFrame({\n",
    "    'id': range(len(test_predicted_labels)),\n",
    "    'predicted_class': test_predicted_labels,\n",
    "    'true_class': test_true_labels\n",
    "})\n",
    "\n",
    "results_df.to_csv('malware_maml_plus_plus_predictions.csv', index=False)\n",
    "print(\"MAML++ test results saved as malware_maml_plus_plus_predictions.csv\")\n",
    "\n",
    "# Calculate detailed metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_true_labels, test_predicted_labels, \n",
    "                          target_names=['Class 0', 'Class 1', 'Class 2']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_true_labels, test_predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
