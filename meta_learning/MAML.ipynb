{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6d3013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from gdown) (3.19.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/homebrew/lib/python3.11/site-packages (from gdown) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/desmond/Library/Python/3.11/lib/python/site-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests[socks]->gdown) (2025.10.5)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [gdown]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PySocks-1.7.1 beautifulsoup4-4.14.2 gdown-5.2.0 soupsieve-2.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e36ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cpu\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import glob, random\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path to import project modules\n",
    "sys.path.append('../src')\n",
    "from extraction.data_loader import create_dataloaders\n",
    "from extraction.downloader import download_dataset\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"DEVICE = {device}\")\n",
    "\n",
    "# Fix random seeds\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75e1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_way = 3  # 2 abnormal classes + 1 normal (benign) class\n",
    "k_shot = 1\n",
    "q_query = 5\n",
    "input_dim = 1280  # Feature dimension from the standardized dataset\n",
    "train_inner_train_step = 5\n",
    "val_inner_train_step = 5\n",
    "inner_lr = 0.001\n",
    "meta_lr = 0.001\n",
    "meta_batch_size = 16\n",
    "max_epoch = 30\n",
    "eval_batches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "data_preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataloaders: ['train', 'val', 'test_seen', 'test_unseen']\n",
      "Train dataset size: 46729\n",
      "Validation dataset size: 5840\n",
      "Test seen dataset size: 5840\n",
      "Test unseen dataset size: 23081\n"
     ]
    }
   ],
   "source": [
    "# Load dataset using the standardized dataloader\n",
    "load_dotenv()\n",
    "\n",
    "# Download dataset if not exists\n",
    "dataset_dir = '../dataset'\n",
    "if not os.path.exists(dataset_dir):\n",
    "    download_dataset(dir=dataset_dir)\n",
    "\n",
    "features_dir = os.path.join(dataset_dir, 'features')\n",
    "split_csv_path = os.path.join(dataset_dir, 'label_split.csv')\n",
    "\n",
    "# Create standardized dataloaders\n",
    "dataloaders = create_dataloaders(\n",
    "    features_dir=features_dir,\n",
    "    split_csv_path=split_csv_path,\n",
    "    batch_size=1,  # We'll handle batching in MAML\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    generalized=False,\n",
    "    num_workers=0  # Set to 0 for compatibility\n",
    ")\n",
    "\n",
    "print(f\"Available dataloaders: {list(dataloaders.keys())}\")\n",
    "print(f\"Train dataset size: {len(dataloaders['train'].dataset)}\")\n",
    "print(f\"Validation dataset size: {len(dataloaders['val'].dataset)}\")\n",
    "print(f\"Test seen dataset size: {len(dataloaders['test_seen'].dataset)}\")\n",
    "print(f\"Test unseen dataset size: {len(dataloaders['test_unseen'].dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "maml_dataset_wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLDatasetWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper to adapt the standardized dataset for MAML's episodic training.\n",
    "    Creates N-way K-shot tasks from the standardized malware dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, n_way=3, k_shot=1, q_query=5, num_tasks=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataloader: One of the standardized dataloaders (train/val/test)\n",
    "            n_way: Number of classes per task\n",
    "            k_shot: Number of support samples per class\n",
    "            q_query: Number of query samples per class\n",
    "            num_tasks: Number of tasks to generate\n",
    "        \"\"\"\n",
    "        self.dataloader = dataloader\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.q_query = q_query\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # Extract all data and organize by class\n",
    "        self.class_data = self._organize_data_by_class()\n",
    "        self.available_classes = list(self.class_data.keys())\n",
    "        \n",
    "        print(f\"Available classes: {len(self.available_classes)}\")\n",
    "        print(f\"Class distribution: {[(cls, len(samples)) for cls, samples in self.class_data.items()]}\")\n",
    "        \n",
    "        # For MAML, we just need enough classes to form n_way tasks\n",
    "        if len(self.available_classes) < self.n_way:\n",
    "            raise ValueError(f\"Need at least {self.n_way} classes for {self.n_way}-way classification, but only found {len(self.available_classes)}\")\n",
    "        \n",
    "        # Validate we have enough samples per class\n",
    "        min_samples_needed = self.k_shot + self.q_query\n",
    "        for cls, samples in self.class_data.items():\n",
    "            if len(samples) < min_samples_needed:\n",
    "                print(f\"Warning: Class {cls} has only {len(samples)} samples, need {min_samples_needed}. Will use replacement sampling.\")\n",
    "    \n",
    "    def _organize_data_by_class(self):\n",
    "        \"\"\"Organize dataset samples by class label\"\"\"\n",
    "        class_data = {}\n",
    "        \n",
    "        for features, label in self.dataloader:\n",
    "            label_item = label.item()\n",
    "            if label_item not in class_data:\n",
    "                class_data[label_item] = []\n",
    "            class_data[label_item].append(features.squeeze(0))  # Remove batch dimension\n",
    "        \n",
    "        return class_data\n",
    "    \n",
    "    def _sample_task_classes(self):\n",
    "        \"\"\"Sample classes for an n-way task\"\"\"\n",
    "        # Randomly sample n_way classes from available classes\n",
    "        sampled_classes = np.random.choice(self.available_classes, self.n_way, replace=False)\n",
    "        return list(sampled_classes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generate a single N-way K-shot task\"\"\"\n",
    "        # Set seed for reproducibility based on index\n",
    "        np.random.seed(42 + idx)\n",
    "        \n",
    "        # Sample classes for this task\n",
    "        task_classes = self._sample_task_classes()\n",
    "        \n",
    "        task_data = []\n",
    "        \n",
    "        for cls in task_classes:\n",
    "            class_samples = self.class_data[cls]\n",
    "            \n",
    "            # Sample support + query samples\n",
    "            total_needed = self.k_shot + self.q_query\n",
    "            \n",
    "            if len(class_samples) >= total_needed:\n",
    "                selected_indices = np.random.choice(len(class_samples), total_needed, replace=False)\n",
    "            else:\n",
    "                # Sample with replacement if not enough samples\n",
    "                selected_indices = np.random.choice(len(class_samples), total_needed, replace=True)\n",
    "            \n",
    "            selected_samples = [class_samples[i] for i in selected_indices]\n",
    "            task_data.append(torch.stack(selected_samples))\n",
    "        \n",
    "        # Stack all class data: [n_way, k_shot + q_query, feature_dim]\n",
    "        task_tensor = torch.stack(task_data)\n",
    "        \n",
    "        # Reshape to [n_way * (k_shot + q_query), feature_dim]\n",
    "        task_tensor = task_tensor.view(-1, task_tensor.size(-1))\n",
    "        \n",
    "        return task_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6545c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structure Overview for MAML with Standardized Dataloader:\n",
    "\n",
    "# Epoch Level (30 epochs)\n",
    "# │\n",
    "# ├── Meta-batch Level (16 batches per epoch)\n",
    "# │   │\n",
    "# │   ├── Task 1: [Malware A, Malware B, Benign] → [18, 1280] 18 = 3 categories * (1 support + 5 query); 1280 = feature dimension\n",
    "# │   ├── Task 2: [Malware C, Malware D, Benign] → [18, 1280] \n",
    "# │   ├── ...\n",
    "# │   └── Task 16: [Malware X, Malware Y, Benign] → [18, 1280]\n",
    "# │   │\n",
    "# │   └── Meta-batch: [16, 18, 1280]\n",
    "# │\n",
    "# └── How to process each Meta-batch in Solver(MAML Algorithm):\n",
    "#     │\n",
    "#     ├── Split Support/Query Set\n",
    "#     │   ├── Support: [16, 3, 1280]  (3 samples(1 for 3 categories) per task)\n",
    "#     │   └── Query:   [16, 15, 1280] (15 samples(5 for 3 categories) per task)\n",
    "#     │\n",
    "#     ├── Inner Training (5 steps, based on Support Set)\n",
    "#     │   └── Fast Adaptation: θ → θ'\n",
    "#     │\n",
    "#     ├── Outer Validation (based on Query Set)\n",
    "#     │   └── Compute meta-loss and accuracy\n",
    "#     │\n",
    "#     └── Outer Update (Meta-gradient)\n",
    "#         └── Update original parameters: θ ← θ - β∇_θ L_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9807cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for labels and accuracy\n",
    "def create_malware_label(k_shot, q_query):\n",
    "    \"\"\"\n",
    "    Create labels for calculating accuracy in test phase.\n",
    "    3 classes: 2 malware + 1 benign\n",
    "    \"\"\"\n",
    "    n_way = 3  # 2 malware + 1 benign\n",
    "    labels = []\n",
    "    for class_idx in range(n_way):\n",
    "        class_labels = [class_idx] * (k_shot + q_query)\n",
    "        labels.extend(class_labels)\n",
    "    \n",
    "    return torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def create_label(n_way, k_shot):\n",
    "    \"\"\"\n",
    "    Create labels for support set and query set.\n",
    "    \"\"\"\n",
    "    return torch.arange(n_way).repeat_interleave(k_shot).long()\n",
    "\n",
    "def calculate_accuracy(logits, labels):\n",
    "    \"\"\"utility function for accuracy calculation\"\"\"\n",
    "    acc = np.asarray(\n",
    "        [(torch.argmax(logits, -1).cpu().numpy() == labels.cpu().numpy())]\n",
    "    ).mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b70d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=3):\n",
    "        \"\"\"\n",
    "        A simple feedforward neural network for malware classification.\n",
    "        input_dim: 1280 (standardized feature dimension)\n",
    "        output_dim: 3 (2 malware + 1 benign)\n",
    "        \"\"\"\n",
    "        super(MalwareClassifier, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def functional_forward(self, x, params):\n",
    "        for i, (name, module) in enumerate(self.network.named_children()):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                weight_key = f'network.{i}.weight'\n",
    "                bias_key = f'network.{i}.bias'\n",
    "                \n",
    "                x = F.linear(x, params.get(weight_key, module.weight), \n",
    "                           params.get(bias_key, module.bias))\n",
    "            elif isinstance(module, nn.ReLU):\n",
    "                x = F.relu(x)\n",
    "            elif isinstance(module, nn.Dropout):\n",
    "                x = F.dropout(x, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "setup_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MAML-compatible datasets...\n",
      "Actual number of classes: 6\n",
      "Available classes: 6\n",
      "Class distribution: [(9, 8536), (2, 8176), (6, 7215), (0, 6712), (1, 9509), (7, 6581)]\n",
      "Available classes: 6\n",
      "Class distribution: [(7, 837), (2, 1030), (1, 1185), (6, 880), (9, 1048), (0, 860)]\n",
      "Available classes: 4\n",
      "Class distribution: [(4, 6760), (8, 6259), (5, 5131), (3, 4931)]\n",
      "Using 3-way classification\n",
      "Train tasks: 1000\n",
      "Validation tasks: 200\n",
      "Test tasks: 300\n"
     ]
    }
   ],
   "source": [
    "# Create MAML-compatible datasets from standardized dataloaders\n",
    "print(\"Creating MAML-compatible datasets...\")\n",
    "\n",
    "# 先檢查可用的類別數量\n",
    "temp_loader = dataloaders['train']\n",
    "temp_classes = set()\n",
    "for _, label in temp_loader:\n",
    "    temp_classes.add(label.item())\n",
    "actual_num_classes = len(temp_classes)\n",
    "print(f\"Actual number of classes: {actual_num_classes}\")\n",
    "\n",
    "# 調整 n_way 以符合可用類別\n",
    "n_way = min(n_way, actual_num_classes)\n",
    "\n",
    "train_maml_dataset = MAMLDatasetWrapper(\n",
    "    dataloaders['train'], \n",
    "    n_way=n_way, \n",
    "    k_shot=k_shot, \n",
    "    q_query=q_query, \n",
    "    num_tasks=1000\n",
    ")\n",
    "\n",
    "val_maml_dataset = MAMLDatasetWrapper(\n",
    "    dataloaders['val'], \n",
    "    n_way=n_way, \n",
    "    k_shot=k_shot, \n",
    "    q_query=q_query, \n",
    "    num_tasks=200\n",
    ")\n",
    "\n",
    "test_maml_dataset = MAMLDatasetWrapper(\n",
    "    dataloaders['test_unseen'], \n",
    "    n_way=n_way, \n",
    "    k_shot=k_shot, \n",
    "    q_query=q_query, \n",
    "    num_tasks=300\n",
    ")\n",
    "\n",
    "# Create DataLoaders for MAML\n",
    "train_loader = DataLoader(train_maml_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_maml_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_maml_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Using {n_way}-way classification\")\n",
    "print(f\"Train tasks: {len(train_maml_dataset)}\")\n",
    "print(f\"Validation tasks: {len(val_maml_dataset)}\")\n",
    "print(f\"Test tasks: {len(test_maml_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "get_meta_batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_batch(meta_batch_size, k_shot, q_query, data_loader, iterator):\n",
    "    \"\"\"\n",
    "    Get meta batch function adapted for standardized dataloader\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _ in range(meta_batch_size):\n",
    "        try:\n",
    "            task_data = next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(data_loader)\n",
    "            task_data = next(iterator)\n",
    "        \n",
    "        # task_data shape: [1, n_way * (k_shot + q_query), feature_dim]\n",
    "        # Remove the batch dimension\n",
    "        task_data = task_data.squeeze(0)  # [n_way * (k_shot + q_query), feature_dim]\n",
    "        data.append(task_data)\n",
    "    \n",
    "    return torch.stack(data).to(device), iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "solver_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main MAML Algorithm (unchanged - works with any properly formatted data)\n",
    "def Solver(\n",
    "    model,\n",
    "    optimizer,\n",
    "    x,\n",
    "    n_way,\n",
    "    k_shot,\n",
    "    q_query,\n",
    "    loss_fn,\n",
    "    inner_train_step,\n",
    "    inner_lr,\n",
    "    train,\n",
    "    return_labels=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main MAML algorithm\n",
    "    \"\"\"\n",
    "    criterion = loss_fn\n",
    "    task_loss = []\n",
    "    task_acc = []\n",
    "    labels = []\n",
    "    \n",
    "    for meta_batch in x:\n",
    "        # Split support and query sets\n",
    "        support_set = meta_batch[: n_way * k_shot]\n",
    "        query_set = meta_batch[n_way * k_shot :]\n",
    "\n",
    "        # Copy the params for inner loop\n",
    "        fast_weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "        ### ---------- INNER TRAIN LOOP ---------- ###\n",
    "        for inner_step in range(inner_train_step):\n",
    "            # Simply training\n",
    "            train_label = create_label(n_way, k_shot).to(device)\n",
    "            logits = model.functional_forward(support_set, fast_weights)\n",
    "            loss = criterion(logits, train_label)\n",
    "            # Inner gradients update!\n",
    "            # Calculate gradients\n",
    "            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n",
    "\n",
    "            # Update fast_weights\n",
    "            # θ' = θ - α * ∇loss\n",
    "            fast_weights = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights.items(), grads)\n",
    "            )\n",
    "\n",
    "        ### ---------- INNER VALID LOOP ---------- ###\n",
    "        if not return_labels:\n",
    "            \"\"\" training / validation \"\"\"\n",
    "            val_label = create_label(n_way, q_query).to(device)\n",
    "\n",
    "            # Collect gradients for outer loop\n",
    "            logits = model.functional_forward(query_set, fast_weights)\n",
    "            loss = criterion(logits, val_label)\n",
    "            task_loss.append(loss)\n",
    "            task_acc.append(calculate_accuracy(logits, val_label))\n",
    "        else:\n",
    "            \"\"\" testing \"\"\"\n",
    "            logits = model.functional_forward(query_set, fast_weights)\n",
    "            labels.extend(torch.argmax(logits, -1).cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return labels\n",
    "\n",
    "    # Update outer loop\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    meta_batch_loss = torch.stack(task_loss).mean()\n",
    "    if train:\n",
    "        \"\"\" Outer Loop Update \"\"\"\n",
    "        # φ backpropagation\n",
    "        meta_batch_loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    task_acc = np.mean(task_acc)\n",
    "    return meta_batch_loss, task_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04971fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427011\n",
      "Using standardized malware dataset with 1280 features\n",
      "Task configuration: 3-way 1-shot with 5 query samples\n"
     ]
    }
   ],
   "source": [
    "# Create model with correct output dimension\n",
    "meta_model = MalwareClassifier(input_dim=input_dim, output_dim=n_way).to(device)\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), lr=meta_lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in meta_model.parameters())}\")\n",
    "print(f\"Using standardized malware dataset with {input_dim} features\")\n",
    "print(f\"Task configuration: {n_way}-way {k_shot}-shot with {q_query} query samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "training_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with standardized dataloader...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:07<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.104\tAccuracy: 33.656%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.333%\n",
      "--------------------------------------------------\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.101\tAccuracy: 33.387%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.576%\n",
      "--------------------------------------------------\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.099\tAccuracy: 34.583%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 31.840%\n",
      "--------------------------------------------------\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.100\tAccuracy: 33.696%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 32.639%\n",
      "--------------------------------------------------\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:05<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.098\tAccuracy: 34.819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.090%\n",
      "--------------------------------------------------\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.098\tAccuracy: 34.704%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 34.062%\n",
      "--------------------------------------------------\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.098\tAccuracy: 34.469%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 22.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.715%\n",
      "--------------------------------------------------\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.097\tAccuracy: 34.738%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 22.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.611%\n",
      "--------------------------------------------------\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.098\tAccuracy: 33.763%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.681%\n",
      "--------------------------------------------------\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.097\tAccuracy: 34.570%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.854%\n",
      "--------------------------------------------------\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.096\tAccuracy: 34.489%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.889%\n",
      "--------------------------------------------------\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.096\tAccuracy: 34.980%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 32.917%\n",
      "--------------------------------------------------\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.094\tAccuracy: 35.094%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 32.951%\n",
      "--------------------------------------------------\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.095\tAccuracy: 34.926%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.646%\n",
      "--------------------------------------------------\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.093\tAccuracy: 35.773%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.090%\n",
      "--------------------------------------------------\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.094\tAccuracy: 34.577%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 32.431%\n",
      "--------------------------------------------------\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.093\tAccuracy: 35.020%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 32.986%\n",
      "--------------------------------------------------\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.092\tAccuracy: 35.524%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.924%\n",
      "--------------------------------------------------\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.091\tAccuracy: 35.101%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.924%\n",
      "--------------------------------------------------\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.091\tAccuracy: 35.652%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 34.306%\n",
      "--------------------------------------------------\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.089\tAccuracy: 35.847%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.507%\n",
      "--------------------------------------------------\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.088\tAccuracy: 35.517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 32.222%\n",
      "--------------------------------------------------\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.088\tAccuracy: 36.364%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 34.514%\n",
      "--------------------------------------------------\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.087\tAccuracy: 36.223%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.715%\n",
      "--------------------------------------------------\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.085\tAccuracy: 35.746%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.229%\n",
      "--------------------------------------------------\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.085\tAccuracy: 36.277%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.090%\n",
      "--------------------------------------------------\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.080\tAccuracy: 36.512%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 20.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.507%\n",
      "--------------------------------------------------\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:05<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.082\tAccuracy: 36.411%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.194%\n",
      "--------------------------------------------------\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.081\tAccuracy: 37.056%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 34.340%\n",
      "--------------------------------------------------\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62/62 [00:06<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.080\tAccuracy: 36.835%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:00<00:00, 21.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.264%\n",
      "--------------------------------------------------\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "train_iter = iter(train_loader)\n",
    "val_iter = iter(val_loader)\n",
    "\n",
    "print(\"Starting training with standardized dataloader...\")\n",
    "for epoch in range(max_epoch):\n",
    "    print(f\"Epoch {epoch+1}/{max_epoch}\")\n",
    "    \n",
    "    # Training\n",
    "    train_meta_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for train_step in tqdm(range(len(train_loader) // meta_batch_size), desc=\"Training\"):\n",
    "        x, train_iter = get_meta_batch(\n",
    "            meta_batch_size, k_shot, q_query, train_loader, train_iter\n",
    "        )\n",
    "        \n",
    "        meta_loss, acc = Solver(\n",
    "            meta_model,\n",
    "            optimizer,\n",
    "            x,\n",
    "            n_way,\n",
    "            k_shot,\n",
    "            q_query,\n",
    "            loss_fn,\n",
    "            inner_train_step=train_inner_train_step,\n",
    "            inner_lr=inner_lr,\n",
    "            train=True,\n",
    "        )\n",
    "        \n",
    "        train_meta_loss.append(meta_loss.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    print(f\"Loss: {np.mean(train_meta_loss):.3f}\\tAccuracy: {np.mean(train_acc)*100:.3f}%\")\n",
    "    \n",
    "    # Validation\n",
    "    val_acc = []\n",
    "    for eval_step in tqdm(range(min(eval_batches, len(val_loader) // meta_batch_size)), desc=\"Validation\"):\n",
    "        x, val_iter = get_meta_batch(\n",
    "            meta_batch_size, k_shot, q_query, val_loader, val_iter\n",
    "        )\n",
    "        \n",
    "        _, acc = Solver(\n",
    "            meta_model,\n",
    "            optimizer,\n",
    "            x,\n",
    "            n_way,\n",
    "            k_shot,\n",
    "            q_query,\n",
    "            loss_fn,\n",
    "            inner_train_step=val_inner_train_step,\n",
    "            inner_lr=inner_lr,\n",
    "            train=False,\n",
    "        )\n",
    "        val_acc.append(acc)\n",
    "    \n",
    "    print(f\"Validation accuracy: {np.mean(val_acc)*100:.3f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "save_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as malware_maml_model_standardized.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': meta_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'hyperparameters': {\n",
    "        'n_way': n_way,\n",
    "        'k_shot': k_shot,\n",
    "        'q_query': q_query,\n",
    "        'input_dim': input_dim,\n",
    "        'inner_lr': inner_lr,\n",
    "        'meta_lr': meta_lr\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'features_dir': features_dir,\n",
    "        'split_csv_path': split_csv_path,\n",
    "        'train_tasks': len(train_maml_dataset),\n",
    "        'val_tasks': len(val_maml_dataset),\n",
    "        'test_tasks': len(test_maml_dataset)\n",
    "    }\n",
    "}, 'malware_maml_model_standardized.pth')\n",
    "\n",
    "print(\"Model saved as malware_maml_model_standardized.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "testing_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_standardized(model, test_loader, inner_train_step=5):\n",
    "    \"\"\"\n",
    "    Test function using standardized dataloader\n",
    "    Returns predicted and true labels for accuracy calculation\n",
    "    \"\"\"\n",
    "    test_iter = iter(test_loader)\n",
    "    \n",
    "    test_batches = min(20, len(test_loader))\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "    task_accuracies = []\n",
    "\n",
    "    print(\"Starting testing with standardized dataloader...\")\n",
    "\n",
    "    for batch_idx in tqdm(range(test_batches), desc=\"Testing\"):\n",
    "        x, test_iter = get_meta_batch(1, k_shot, q_query, test_loader, test_iter)\n",
    "\n",
    "        # 3-way task query set labels (0, 1, 2 for each class)\n",
    "        task_true_labels = []\n",
    "        for class_idx in range(n_way):\n",
    "            task_true_labels.extend([class_idx] * q_query)\n",
    "\n",
    "        # Get model predictions\n",
    "        predicted_labels = Solver(\n",
    "            model,\n",
    "            optimizer,\n",
    "            x,\n",
    "            n_way,\n",
    "            k_shot,\n",
    "            q_query,\n",
    "            loss_fn,\n",
    "            inner_train_step=inner_train_step,\n",
    "            inner_lr=inner_lr,\n",
    "            train=False,\n",
    "            return_labels=True,\n",
    "        )\n",
    "\n",
    "        # Calculate current task accuracy\n",
    "        task_true = np.array(task_true_labels)\n",
    "        task_pred = np.array(predicted_labels)\n",
    "        task_acc = (task_true == task_pred).mean()\n",
    "        task_accuracies.append(task_acc)\n",
    "\n",
    "        # Collect all labels\n",
    "        all_predicted_labels.extend(predicted_labels)\n",
    "        all_true_labels.extend(task_true_labels)\n",
    "\n",
    "        if batch_idx % 5 == 0:  # Print every 5 batches\n",
    "            print(f\"Batch {batch_idx+1}/{test_batches} - Task Accuracy: {task_acc:.4f}\")\n",
    "    \n",
    "    return all_predicted_labels, all_true_labels, task_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "run_testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing with standardized dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 20/20 [00:00<00:00, 150.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/20 - Task Accuracy: 0.4667\n",
      "Batch 6/20 - Task Accuracy: 0.4667\n",
      "Batch 11/20 - Task Accuracy: 0.4667\n",
      "Batch 16/20 - Task Accuracy: 0.3333\n",
      "Average Test Task Accuracy: 37.333%\n",
      "Total test samples: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute testing with standardized dataloader\n",
    "test_predicted_labels, test_true_labels, test_task_accuracies = test_model_standardized(\n",
    "    meta_model, test_loader\n",
    ")\n",
    "average_test_accuracy = np.mean(test_task_accuracies)\n",
    "print(f\"Average Test Task Accuracy: {average_test_accuracy*100:.3f}%\")\n",
    "print(f\"Total test samples: {len(test_predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results saved as malware_maml_predictions_standardized.csv\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Malware_A       0.36      0.55      0.44       100\n",
      "   Malware_B       0.44      0.29      0.35       100\n",
      "      Benign       0.34      0.28      0.31       100\n",
      "\n",
      "    accuracy                           0.37       300\n",
      "   macro avg       0.38      0.37      0.36       300\n",
      "weighted avg       0.38      0.37      0.36       300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55 18 27]\n",
      " [43 29 28]\n",
      " [53 19 28]]\n"
     ]
    }
   ],
   "source": [
    "# Save test results\n",
    "results_df = pd.DataFrame({\n",
    "    'id': range(len(test_predicted_labels)),\n",
    "    'predicted_class': test_predicted_labels,\n",
    "    'true_class': test_true_labels\n",
    "})\n",
    "\n",
    "results_df.to_csv('malware_maml_predictions_standardized.csv', index=False)\n",
    "print(\"Test results saved as malware_maml_predictions_standardized.csv\")\n",
    "\n",
    "# Calculate and print additional metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_true_labels, test_predicted_labels, \n",
    "                          target_names=['Malware_A', 'Malware_B', 'Benign']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_true_labels, test_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verification of Standardized Dataloader Integration ===\n",
      "Dataset directory: ../dataset\n",
      "Features directory: ../dataset/features\n",
      "Split CSV path: ../dataset/label_split.csv\n",
      "Available dataloaders: ['train', 'val', 'test_seen', 'test_unseen']\n",
      "Feature dimension: 1280\n",
      "Task configuration: 3-way 1-shot classification\n",
      "Model trained for 30 epochs\n",
      "Final test accuracy: 37.333%\n",
      "\n",
      "Integration with standardized dataloader: SUCCESS ✓\n"
     ]
    }
   ],
   "source": [
    "# Verification: Check that we're using the standardized dataloader correctly\n",
    "print(\"=== Verification of Standardized Dataloader Integration ===\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Features directory: {features_dir}\")\n",
    "print(f\"Split CSV path: {split_csv_path}\")\n",
    "print(f\"Available dataloaders: {list(dataloaders.keys())}\")\n",
    "print(f\"Feature dimension: {input_dim}\")\n",
    "print(f\"Task configuration: {n_way}-way {k_shot}-shot classification\")\n",
    "print(f\"Model trained for {max_epoch} epochs\")\n",
    "print(f\"Final test accuracy: {average_test_accuracy*100:.3f}%\")\n",
    "print(\"\\nIntegration with standardized dataloader: SUCCESS ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
